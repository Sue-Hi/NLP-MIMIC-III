{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Approach\n",
    "In this notebook I am going to use 2 deep learning approaches:\n",
    "\n",
    "* Collobert et al.(2011) and Gehrmann et al. (2017) CNN model. This model has shown a good performance on text analysis specifically MIMIC-III discharge summaries.\n",
    "        \n",
    "* CNN-BiLSTM: I am using this approach due to multiple reasons:\n",
    "        \n",
    "     * CNN can do feature extraction also it improves the result and speed of LSTM model which is a good method for analyzing sequence data\n",
    "     * BiLSTMs can understand context better than LSTM\n",
    "       \n",
    "    \n",
    "I am also trying different embedding methods:\n",
    "\n",
    "* Make my own embedding using more than 2 million notes that are available in MIMIC-III database.\n",
    "\n",
    " * GloVe pretrained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import *\n",
    "from nltk.util import ngrams\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from time import time\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, Bidirectional\n",
    "from keras.layers import MaxPooling1D, Dropout, Activation, GlobalMaxPooling1D, Add, Concatenate, concatenate, Input\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import Adam, Adadelta\n",
    "\n",
    "# SkLearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from keras.callbacks import TensorBoard\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle('df_train.pkl')\n",
    "df_valid = pd.read_pickle('df_valid.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(df):\n",
    "    # This function preprocesses the text by filling not a number and replacing new lines ('\\n') and carriage returns ('\\r')\n",
    "    df.TEXT = df.TEXT.fillna(' ')\n",
    "    df.TEXT = df.TEXT.str.replace('\\n',' ')\n",
    "    df.TEXT = df.TEXT.str.replace('\\r',' ')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = preprocess_text(df_train)\n",
    "df_valid = preprocess_text(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train.OUTPUT_LABEL\n",
    "y_valid = df_valid.OUTPUT_LABEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I am not going to do stemming this time, as embedding of different formats of a word are very close to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\n",
    "    punc_list = string.punctuation+'0123456789'\n",
    "    t = str.maketrans(dict.fromkeys(punc_list, \" \"))\n",
    "    text = text.lower().translate(t).split()\n",
    "    \n",
    "    ## Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    ## Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    ## Stemming\n",
    "    # text = text.split()\n",
    "    # stemmer = SnowballStemmer('english')\n",
    "    # stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    # text = \" \".join(stemmed_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['TEXT'] = df_train['TEXT'].map(lambda x: clean_text(x))\n",
    "df_valid['TEXT'] = df_valid['TEXT'].map(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'admission date discharge date date birth sex service colorectal surgery green surgery history present illness year old man history ulcerative colitis since patient hospitalized almost annually flareups current flare began three weeks ago time admitted hospital past three weeks recently started hydrocortisone sent home several days prior admission patient complained increasing symptoms weekend severe lower abdominal pain intake low grade fevers nausea vomiting bloody bowel movements per day past medical history ulcerative colitis past surgical history none medications hydrocortisone tid two ativan prn iron folic acid prevacid allergies mercaptopurine reaction jaundice social history tobacco occasional alcohol family history mother name disease review systems chest pain shortness breath palpitations dysuria hematuria hematemesis physical examination admission vital signs temperature heart rate blood pressure respirations pulse oxygenation room air alert oriented times three acute distress sclerae anicteric mucous membranes moist heart rate regular rate rhythm murmurs rubs gallops lungs clear auscultation bilaterally abdomen soft tender lower quadrants palpation guarding positive bowel sounds rectal examination grossly heme positive positive external hemorrhoid visualized extremities warm well perfused edema scan abdomen admission showed evidence free air obstruction abscess diffuse colonic thickening loss haustral folds multiple nodular filling defects transverse colon please see full report details laboratories admission complete blood count follows white blood cell count hematocrit platelet count white blood cell count differential neutrophils band neutrophils lymphocytes monocytes electrolytes follows sodium potassium chloride hco bun creatinine glucose patient admitted colorectal service last name stitle written diet nothing mouth fluids medicated steroids antibiotics given routine preoperative assessment electrocardiogram chest ray postoperative day two patient received peripherally inserted central catheter line administration total parenteral nutrition started morphine sulfate pca pain control visited enterostomal nurse therapist education discussion ileostomy care hospital day four patient taken operating room restorative proctocolectomy diverting ileostomy please see full operative report details procedure following procedure patient hypotensive elevated heart rate decreased urine output infused lactated ringers well hespan volume resuscitation urine output responded marginally boluses patient postoperative hematocrit electrolytes within normal limits except magnesium given grams magnesium intravenously several hours time postoperatively patient noted dysnomia difficulty speaking neurology consult obtained time please see full neurology consult note details scan head obtained abnormalities noted patient transferred surgical intensive care unit team care monitoring volume resuscitation neo synephrine drip postoperative day one patient blood pressure stabilized patient taken mri evaluation speech difficulties mri suggestive acute left temporal infarct mass effect midline shift acute occlusion please see full mri report details patient worked cause left temporal infarct transesophageal echocardiogram noted small atrioseptal defect right left flow clinically patient aphasia improving colostomy viable putting small amounts liquid brown stool patient remained total parenteral nutrition consultation nutritionist staff patient seen last name stitle evaluation closure atrioseptal defect hospital day postoperative day six patient deemed stable enough return surgical floor transferred intensive care unit able tolerate regular diet pain well controlled able ambulate neurological changes complaints postoperative day eight deemed stable enough condition transfer home visiting nurse services addendum patient underwent colonoscopy hospital day two showed severe ulcerations colon please see full colonoscopy report details procedure discharge diagnosis ulcerative colitis primary status post restorative proctocolectomy diverting ileostomy left temporal lobe cerebral infarct atrioseptal defect secondary hypotension hypovolemia condition discharge good stable discharge status home visiting nurses discharge medications aspirin tablet one tablet day clopidogrel tablet one tablet day tylenol tablets needed pain loperamide one capsule qid prednisone tablets three tablets day week two tablets followup last name stitle pravastatin tablet one tablet day follow plans patient followup last name stitle colorectal surgery weeks office number call appointment last name stitle interventional cardiology repair atrioseptal defect patient given office number call appointment addition patient referred visiting nurses association services dressing changes dry gauze twice day well ostomy care routine twice day instructed take regular diet regular activity tolerated first name name pattern last name namepattern dictated last name namepattern medquist job job number'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.TEXT[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now I want to create sequence of the words using Keras Tokenizer. I only consider the first 4000 words of the texts. This is more than enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_leng = 4000\n",
    "vocabulary_size = 400000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(df_train['TEXT'])\n",
    "sequences = tokenizer.texts_to_sequences(df_train['TEXT'])\n",
    "data = pad_sequences(sequences, maxlen=max_leng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create sequence for validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_valid = tokenizer.texts_to_sequences(df_valid['TEXT'])\n",
    "data_valid = pad_sequences(sequences_valid , maxlen=max_leng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  450,  450,  171],\n",
       "       [   0,    0,    0, ...,  450,  450,  171],\n",
       "       [   0,    0,    0, ...,  450,  450,  171],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  170,   60, 1092],\n",
       "       [   0,    0,    0, ...,    4,  231,  275],\n",
       "       [   0,    0,    0, ...,    2,   20,  171]], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = []\n",
    "AUC = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Embedding:\n",
    "#### I am using gensim Word2Vec function to train a corpus of 2 million notes available in MIMIC-III NOTEEVENTS table. To prevent any leakage of data I have removed the test dataset notes from this corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vec = Word2Vec.load(\"Word2Vec_5.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let’s see what the embedding of  \"admission\" looks like in a 100 dimensions world.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.940672  ,  4.7706604 , -0.17481485,  2.2851875 ,  3.879398  ,\n",
       "       -0.34852552,  4.117314  , -3.8222647 ,  2.6205635 , -0.90670586,\n",
       "       -2.7382095 ,  3.1581028 ,  5.8051367 ,  3.2299984 , -1.6231452 ,\n",
       "        4.0764256 , -2.7143908 , -4.548696  ,  2.6629424 , -1.9236807 ,\n",
       "        2.9011989 , -0.9748698 , -0.21091324, -0.12038308,  0.15923713,\n",
       "       -8.00361   , -0.7134761 ,  0.8393403 , -1.629703  , -3.187114  ,\n",
       "       -0.40883386, -5.453877  , -5.1056876 , -1.3243814 ,  1.9521133 ,\n",
       "       -1.1441629 , -6.0349827 , -1.7179009 ,  0.8746022 ,  3.4964767 ,\n",
       "        4.7658687 , -1.5917919 ,  1.0501578 , -4.5565786 , -0.37815642,\n",
       "       -3.9397233 , -3.086613  , -2.6896765 , -3.4436028 , -2.5318625 ,\n",
       "        1.2601362 , -1.2332696 , -3.9269254 ,  1.434145  ,  0.15418415,\n",
       "        2.2525094 ,  2.9194953 , -3.1198769 ,  0.9414409 ,  0.23927784,\n",
       "        0.6692262 , -1.4693029 , -4.6392746 ,  1.5093796 ,  5.982255  ,\n",
       "       -6.322537  ,  2.7222247 , -2.4184532 ,  0.60186446, -1.1900263 ,\n",
       "        2.2218902 , -4.448456  , -0.9070071 , -3.0830328 , -2.8738463 ,\n",
       "        5.4960814 , -0.46067753,  0.22246265,  1.5152141 ,  0.13105275,\n",
       "       -1.332013  ,  3.3758743 , -8.937903  , -2.8726144 ,  1.4608823 ,\n",
       "       -0.2637098 , -1.9143552 , -4.230375  , -1.2410824 ,  5.6244807 ,\n",
       "        4.5168705 ,  4.011388  ,  4.1665163 , -2.8069725 , -3.1531224 ,\n",
       "       -7.575724  ,  0.79564524, -2.4157934 , -2.2183423 ,  2.6628287 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vec['admission']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(tokenizer.word_index)+1, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = model_vec[word]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = embedding_matrix.shape[0]\n",
    "max_leng = 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Collobert et al. Model (2011) with Word2Vec Embedding\n",
    "#### This model that is later used by Gehrmann et al. (2017) has shown a good performance on MIMIC-III discharge summaries ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(max_leng, vocabulary_size):\n",
    "    inputs = Input(shape=(max_leng,))\n",
    "    # channel 1\n",
    "    embedding1 = Embedding(vocabulary_size, 100, input_length=max_leng , weights=[embedding_matrix], trainable=False)(inputs)\n",
    "    conv1 = Conv1D(filters=100, kernel_size = 1, activation='relu')(embedding1)\n",
    "    drop1 = Dropout(0.5)(conv1)\n",
    "    pool1 = GlobalMaxPooling1D()(drop1)\n",
    "    #flat1 = Flatten()(pool1)\n",
    "    # channel 2\n",
    "    embedding2 = Embedding(vocabulary_size, 100, input_length=max_leng , weights=[embedding_matrix], trainable=False)(inputs)\n",
    "    conv2 = Conv1D(filters=100, kernel_size = 2, activation='relu')(embedding2)\n",
    "    drop2 = Dropout(0.5)(conv2)\n",
    "    pool2 = GlobalMaxPooling1D()(drop2)\n",
    "    #flat2 = Flatten()(pool2)\n",
    "    # channel 3\n",
    "    embedding3 = Embedding(vocabulary_size, 100, input_length=max_leng , weights=[embedding_matrix], trainable=False)(inputs)\n",
    "    conv3 = Conv1D(filters=100, kernel_size = 3, activation='relu')(embedding3)\n",
    "    drop3 = Dropout(0.5)(conv3)\n",
    "    pool3 = GlobalMaxPooling1D()(drop3)\n",
    "    #flat3 = Flatten()(pool3)\n",
    "    # channel 3\n",
    "    embedding4 = Embedding(vocabulary_size, 100, input_length=max_leng , weights=[embedding_matrix], trainable=False)(inputs)\n",
    "    conv4 = Conv1D(filters=100, kernel_size = 4, activation='relu')(embedding3)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = GlobalMaxPooling1D()(drop4)\n",
    "    #flat4 = Flatten()(pool4)\n",
    "    # channel 3\n",
    "    embedding5 = Embedding(vocabulary_size, 100, input_length=max_leng , weights=[embedding_matrix], trainable=False)(inputs)\n",
    "    conv5 = Conv1D(filters=100, kernel_size = 5, activation='relu')(embedding3)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "    pool5 = GlobalMaxPooling1D()(drop5)\n",
    "    #flat5 = Flatten()(pool5)\n",
    "    # merge\n",
    "    merged = concatenate([pool1, pool2, pool3, pool4, pool5])\n",
    "    #dense1 = Dense(hidden_dims, activation='relu')(merged)\n",
    "    outputs = Dense(1, activation='sigmoid')(merged)\n",
    "    model = Model(inputs=[inputs], outputs=outputs)\n",
    "    # compile\n",
    "    adam = Adam(0.0001)\n",
    "    # adadelta = Adadelta(0.0001)\n",
    "    model.compile(loss='binary_crossentropy', optimizer= 'adam', metrics=['accuracy'])\n",
    "    # summarize\n",
    "    print(model.summary())\n",
    "    #plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 4000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 4000, 100)    4090300     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 4000, 100)    4090300     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 4000, 100)    4090300     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 4000, 100)    10100       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 3999, 100)    20100       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 3998, 100)    30100       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 3997, 100)    40100       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 3996, 100)    50100       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 4000, 100)    0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 3999, 100)    0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 3998, 100)    0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 3997, 100)    0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 3996, 100)    0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 100)          0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 100)          0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 100)          0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 100)          0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 100)          0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 500)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            501         concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 12,421,901\n",
      "Trainable params: 151,001\n",
      "Non-trainable params: 12,270,900\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "# define model\n",
    "model = define_model(max_leng, vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4784 samples, validate on 10223 samples\n",
      "Epoch 1/10\n",
      "4784/4784 [==============================] - 173s 36ms/step - loss: 0.1641 - acc: 0.9500 - val_loss: 0.9521 - val_acc: 0.7570\n",
      "Epoch 2/10\n",
      "4784/4784 [==============================] - 173s 36ms/step - loss: 0.1915 - acc: 0.9465 - val_loss: 1.3552 - val_acc: 0.6659\n",
      "Epoch 3/10\n",
      "4784/4784 [==============================] - 174s 36ms/step - loss: 0.1750 - acc: 0.9498 - val_loss: 3.3121 - val_acc: 0.3911\n",
      "Epoch 4/10\n",
      "4784/4784 [==============================] - 175s 37ms/step - loss: 0.1565 - acc: 0.9528 - val_loss: 1.6618 - val_acc: 0.6240\n",
      "Epoch 5/10\n",
      "4784/4784 [==============================] - 176s 37ms/step - loss: 0.2129 - acc: 0.9454 - val_loss: 2.2235 - val_acc: 0.5442\n",
      "Epoch 6/10\n",
      "4784/4784 [==============================] - 179s 37ms/step - loss: 0.2086 - acc: 0.9463 - val_loss: 1.3259 - val_acc: 0.6762\n",
      "Epoch 7/10\n",
      "4784/4784 [==============================] - 176s 37ms/step - loss: 0.1901 - acc: 0.9498 - val_loss: 6.0634 - val_acc: 0.2424\n",
      "Epoch 8/10\n",
      "4784/4784 [==============================] - 176s 37ms/step - loss: 0.2386 - acc: 0.9365 - val_loss: 1.0611 - val_acc: 0.7382\n",
      "Epoch 9/10\n",
      "4784/4784 [==============================] - 176s 37ms/step - loss: 0.1108 - acc: 0.9638 - val_loss: 0.9041 - val_acc: 0.7640\n",
      "Epoch 10/10\n",
      "4784/4784 [==============================] - 174s 36ms/step - loss: 0.1155 - acc: 0.9676 - val_loss: 2.1122 - val_acc: 0.5728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd80211e2e8>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data,np.array(y_train),validation_data=(data_valid, y_valid), epochs= 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6839439830314257"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(data_valid)\n",
    "fpr, tpr, _ = metrics.roc_curve(np.array(y_valid), y_pred)\n",
    "auc = metrics.auc(fpr,tpr)\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('CNN5_W2V')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model.append('Collbert CNN Model with Word2Vec Embedding')\n",
    "AUC.append(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN + Stacked Bidirectional LSTM and Word2Vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv_model():\n",
    "    model_conv = Sequential()\n",
    "    model_conv.add(Embedding(vocabulary_size, 100, input_length=max_leng , weights=[embedding_matrix], trainable=False))\n",
    "    model_conv.add(Dropout(0.2))\n",
    "    model_conv.add(Conv1D(64, 5, activation='relu'))\n",
    "    model_conv.add(MaxPooling1D(pool_size=4))\n",
    "    model_conv.add(Bidirectional(LSTM(100,recurrent_dropout= 0.2, return_sequences= True)))\n",
    "    model_conv.add(Bidirectional(LSTM(50,recurrent_dropout= 0.2)))\n",
    "    model_conv.add(Dense(1, activation='sigmoid'))\n",
    "    model_conv.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model_conv\n",
    "model_conv = create_conv_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4784 samples, validate on 10223 samples\n",
      "Epoch 1/10\n",
      "4784/4784 [==============================] - 452s 94ms/step - loss: 0.6659 - acc: 0.5826 - val_loss: 0.6226 - val_acc: 0.5989\n",
      "Epoch 2/10\n",
      "4784/4784 [==============================] - 451s 94ms/step - loss: 0.6527 - acc: 0.6125 - val_loss: 0.6962 - val_acc: 0.4563\n",
      "Epoch 3/10\n",
      "4784/4784 [==============================] - 450s 94ms/step - loss: 0.6392 - acc: 0.6246 - val_loss: 0.6241 - val_acc: 0.5857\n",
      "Epoch 4/10\n",
      "4784/4784 [==============================] - 450s 94ms/step - loss: 0.6262 - acc: 0.6407 - val_loss: 0.6880 - val_acc: 0.5094\n",
      "Epoch 5/10\n",
      "4784/4784 [==============================] - 450s 94ms/step - loss: 0.6019 - acc: 0.6735 - val_loss: 0.5680 - val_acc: 0.7058\n",
      "Epoch 6/10\n",
      "4784/4784 [==============================] - 450s 94ms/step - loss: 0.5877 - acc: 0.6802 - val_loss: 0.6989 - val_acc: 0.5592\n",
      "Epoch 7/10\n",
      "4784/4784 [==============================] - 451s 94ms/step - loss: 0.5658 - acc: 0.7026 - val_loss: 0.6758 - val_acc: 0.5743\n",
      "Epoch 8/10\n",
      "4784/4784 [==============================] - 450s 94ms/step - loss: 0.5235 - acc: 0.7393 - val_loss: 0.8431 - val_acc: 0.5131\n",
      "Epoch 9/10\n",
      "4784/4784 [==============================] - 451s 94ms/step - loss: 0.5041 - acc: 0.7481 - val_loss: 0.6217 - val_acc: 0.6703\n",
      "Epoch 10/10\n",
      "4784/4784 [==============================] - 451s 94ms/step - loss: 0.4659 - acc: 0.7761 - val_loss: 0.6797 - val_acc: 0.6502\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd80c1f9a90>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_conv.fit(data, np.array(y_train), validation_data=(data_valid, y_valid), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63704221674103"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model_conv.predict_proba(data_valid)\n",
    "fpr, tpr, _ = metrics.roc_curve(np.array(y_valid), y_pred)\n",
    "auc = metrics.auc(fpr,tpr)\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('CNN_BiLSTM_W2V')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model.append('CNN-BiLSTM Model with Word2Vec Embedding')\n",
    "AUC.append(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe Embedding:\n",
    "#### Here I am using GloVe model with 6B tokens and 400k vocabs from Wikipedia 2014 + Gigaword5 with 100 dimensions. You can download this pretrained embedding here:\n",
    "https://nlp.stanford.edu/projects/glove/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 400000\n",
    "embedding_matrix = np.zeros((vocabulary_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > vocabulary_size - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(max_leng, vocabulary_size):\n",
    "    inputs = Input(shape=(max_leng,))\n",
    "    # channel 1\n",
    "    embedding1 = Embedding(vocabulary_size, 100, input_length=max_leng , weights=[embedding_matrix], trainable=False)(inputs)\n",
    "    conv1 = Conv1D(filters=100, kernel_size = 1, activation='relu')(embedding1)\n",
    "    drop1 = Dropout(0.5)(conv1)\n",
    "    pool1 = GlobalMaxPooling1D()(drop1)\n",
    "    #flat1 = Flatten()(pool1)\n",
    "    # channel 2\n",
    "    embedding2 = Embedding(vocabulary_size, 100, input_length=max_leng , weights=[embedding_matrix], trainable=False)(inputs)\n",
    "    conv2 = Conv1D(filters=100, kernel_size = 2, activation='relu')(embedding2)\n",
    "    drop2 = Dropout(0.5)(conv2)\n",
    "    pool2 = GlobalMaxPooling1D()(drop2)\n",
    "    #flat2 = Flatten()(pool2)\n",
    "    # channel 3\n",
    "    embedding3 = Embedding(vocabulary_size, 100, input_length=max_leng , weights=[embedding_matrix], trainable=False)(inputs)\n",
    "    conv3 = Conv1D(filters=100, kernel_size = 3, activation='relu')(embedding3)\n",
    "    drop3 = Dropout(0.5)(conv3)\n",
    "    pool3 = GlobalMaxPooling1D()(drop3)\n",
    "    #flat3 = Flatten()(pool3)\n",
    "    # channel 3\n",
    "    embedding4 = Embedding(vocabulary_size, 100, input_length=max_leng , weights=[embedding_matrix], trainable=False)(inputs)\n",
    "    conv4 = Conv1D(filters=100, kernel_size = 4, activation='relu')(embedding3)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = GlobalMaxPooling1D()(drop4)\n",
    "    #flat4 = Flatten()(pool4)\n",
    "    # channel 3\n",
    "    embedding5 = Embedding(vocabulary_size, 100, input_length=max_leng , weights=[embedding_matrix], trainable=False)(inputs)\n",
    "    conv5 = Conv1D(filters=100, kernel_size = 5, activation='relu')(embedding3)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "    pool5 = GlobalMaxPooling1D()(drop5)\n",
    "    #flat5 = Flatten()(pool5)\n",
    "    # merge\n",
    "    merged = concatenate([pool1, pool2, pool3, pool4, pool5])\n",
    "    # interpretation\n",
    "    # dense1 = Dense(hidden_dims, activation='relu')(merged)\n",
    "    outputs = Dense(1, activation='sigmoid')(merged)\n",
    "    model = Model(inputs=[inputs], outputs=outputs)\n",
    "    # compile\n",
    "    adam = Adam(0.00001)\n",
    "    # adadelta = Adadelta(0.0001)\n",
    "    model.compile(loss='binary_crossentropy', optimizer= 'adam', metrics=['accuracy'])\n",
    "    # summarize\n",
    "    print(model.summary())\n",
    "    #plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4784 samples, validate on 10223 samples\n",
      "Epoch 1/50\n",
      "4784/4784 [==============================] - 161s 34ms/step - loss: 0.2276 - acc: 0.9446 - val_loss: 1.9136 - val_acc: 0.6089\n",
      "Epoch 2/50\n",
      "4784/4784 [==============================] - 163s 34ms/step - loss: 0.1131 - acc: 0.9638 - val_loss: 1.1625 - val_acc: 0.7331\n",
      "Epoch 3/50\n",
      "4784/4784 [==============================] - 165s 35ms/step - loss: 0.1485 - acc: 0.9565 - val_loss: 0.8930 - val_acc: 0.7827\n",
      "Epoch 4/50\n",
      "4784/4784 [==============================] - 163s 34ms/step - loss: 0.1556 - acc: 0.9540 - val_loss: 1.4379 - val_acc: 0.6707\n",
      "Epoch 5/50\n",
      "4784/4784 [==============================] - 164s 34ms/step - loss: 0.1353 - acc: 0.9605 - val_loss: 0.6316 - val_acc: 0.8612\n",
      "Epoch 6/50\n",
      "4784/4784 [==============================] - 165s 34ms/step - loss: 0.1557 - acc: 0.9555 - val_loss: 2.0349 - val_acc: 0.5890\n",
      "Epoch 7/50\n",
      "4784/4784 [==============================] - 166s 35ms/step - loss: 0.2151 - acc: 0.9461 - val_loss: 1.0655 - val_acc: 0.7554\n",
      "Epoch 8/50\n",
      "4784/4784 [==============================] - 165s 34ms/step - loss: 0.2230 - acc: 0.9425 - val_loss: 1.3116 - val_acc: 0.7137\n",
      "Epoch 9/50\n",
      "4784/4784 [==============================] - 165s 34ms/step - loss: 0.1630 - acc: 0.9551 - val_loss: 0.9688 - val_acc: 0.7774\n",
      "Epoch 10/50\n",
      "4784/4784 [==============================] - 166s 35ms/step - loss: 0.1723 - acc: 0.9578 - val_loss: 2.2417 - val_acc: 0.5711\n",
      "Epoch 11/50\n",
      "4784/4784 [==============================] - 165s 34ms/step - loss: 0.1672 - acc: 0.9511 - val_loss: 0.9263 - val_acc: 0.7978\n",
      "Epoch 12/50\n",
      "4784/4784 [==============================] - 165s 34ms/step - loss: 0.1113 - acc: 0.9647 - val_loss: 1.1033 - val_acc: 0.7476\n",
      "Epoch 13/50\n",
      "4784/4784 [==============================] - 166s 35ms/step - loss: 0.1424 - acc: 0.9611 - val_loss: 1.5158 - val_acc: 0.6831\n",
      "Epoch 14/50\n",
      "4784/4784 [==============================] - 166s 35ms/step - loss: 0.1668 - acc: 0.9559 - val_loss: 0.6683 - val_acc: 0.8685\n",
      "Epoch 15/50\n",
      "4784/4784 [==============================] - 167s 35ms/step - loss: 0.2982 - acc: 0.9312 - val_loss: 1.3210 - val_acc: 0.7369\n",
      "Epoch 16/50\n",
      "4784/4784 [==============================] - 166s 35ms/step - loss: 0.1328 - acc: 0.9611 - val_loss: 2.7064 - val_acc: 0.5430\n",
      "Epoch 17/50\n",
      "4784/4784 [==============================] - 165s 35ms/step - loss: 0.2669 - acc: 0.9381 - val_loss: 0.7899 - val_acc: 0.8374\n",
      "Epoch 18/50\n",
      "4784/4784 [==============================] - 166s 35ms/step - loss: 0.1497 - acc: 0.9588 - val_loss: 1.9077 - val_acc: 0.6452\n",
      "Epoch 19/50\n",
      "4784/4784 [==============================] - 166s 35ms/step - loss: 0.1127 - acc: 0.9699 - val_loss: 2.8044 - val_acc: 0.5409\n",
      "Epoch 20/50\n",
      "4784/4784 [==============================] - 166s 35ms/step - loss: 0.1299 - acc: 0.9668 - val_loss: 2.4871 - val_acc: 0.5767\n",
      "Epoch 21/50\n",
      "4784/4784 [==============================] - 166s 35ms/step - loss: 0.2727 - acc: 0.9446 - val_loss: 2.6284 - val_acc: 0.5534\n",
      "Epoch 22/50\n",
      "4784/4784 [==============================] - 166s 35ms/step - loss: 0.1096 - acc: 0.9693 - val_loss: 4.6750 - val_acc: 0.3835\n",
      "Epoch 23/50\n",
      "4784/4784 [==============================] - 167s 35ms/step - loss: 0.1206 - acc: 0.9680 - val_loss: 1.8849 - val_acc: 0.6545\n",
      "Epoch 24/50\n",
      "4784/4784 [==============================] - 167s 35ms/step - loss: 0.0960 - acc: 0.9720 - val_loss: 1.8781 - val_acc: 0.6567\n",
      "Epoch 25/50\n",
      "4784/4784 [==============================] - 165s 35ms/step - loss: 0.1103 - acc: 0.9695 - val_loss: 2.3935 - val_acc: 0.5829\n",
      "Epoch 26/50\n",
      "4784/4784 [==============================] - 167s 35ms/step - loss: 0.1688 - acc: 0.9597 - val_loss: 2.2572 - val_acc: 0.6146\n",
      "Epoch 27/50\n",
      "4784/4784 [==============================] - 166s 35ms/step - loss: 0.0973 - acc: 0.9691 - val_loss: 1.9477 - val_acc: 0.6600\n",
      "Epoch 28/50\n",
      "4784/4784 [==============================] - 167s 35ms/step - loss: 0.0923 - acc: 0.9732 - val_loss: 1.9927 - val_acc: 0.6441\n",
      "Epoch 29/50\n",
      "4784/4784 [==============================] - 167s 35ms/step - loss: 0.1225 - acc: 0.9657 - val_loss: 5.6777 - val_acc: 0.3274\n",
      "Epoch 30/50\n",
      "4784/4784 [==============================] - 167s 35ms/step - loss: 0.1470 - acc: 0.9615 - val_loss: 1.8436 - val_acc: 0.6731\n",
      "Epoch 31/50\n",
      "4784/4784 [==============================] - 168s 35ms/step - loss: 0.0903 - acc: 0.9730 - val_loss: 1.1215 - val_acc: 0.7752\n",
      "Epoch 32/50\n",
      "4784/4784 [==============================] - 166s 35ms/step - loss: 0.1859 - acc: 0.9555 - val_loss: 6.0794 - val_acc: 0.3181\n",
      "Epoch 33/50\n",
      "4784/4784 [==============================] - 166s 35ms/step - loss: 0.2049 - acc: 0.9548 - val_loss: 3.6622 - val_acc: 0.4871\n",
      "Epoch 34/50\n",
      "4784/4784 [==============================] - 168s 35ms/step - loss: 0.1094 - acc: 0.9691 - val_loss: 3.5514 - val_acc: 0.4889\n",
      "Epoch 35/50\n",
      "4784/4784 [==============================] - 167s 35ms/step - loss: 0.2191 - acc: 0.9503 - val_loss: 1.9089 - val_acc: 0.6767\n",
      "Epoch 36/50\n",
      "4784/4784 [==============================] - 166s 35ms/step - loss: 0.2070 - acc: 0.9551 - val_loss: 1.3925 - val_acc: 0.7526\n",
      "Epoch 37/50\n",
      "4784/4784 [==============================] - 167s 35ms/step - loss: 0.1173 - acc: 0.9697 - val_loss: 1.6519 - val_acc: 0.7068\n",
      "Epoch 38/50\n",
      "4784/4784 [==============================] - 167s 35ms/step - loss: 0.1268 - acc: 0.9668 - val_loss: 1.9569 - val_acc: 0.6613\n",
      "Epoch 39/50\n",
      "4784/4784 [==============================] - 168s 35ms/step - loss: 0.1135 - acc: 0.9701 - val_loss: 4.6090 - val_acc: 0.4175\n",
      "Epoch 40/50\n",
      "4784/4784 [==============================] - 167s 35ms/step - loss: 0.1197 - acc: 0.9653 - val_loss: 1.0948 - val_acc: 0.7969\n",
      "Epoch 41/50\n",
      "4784/4784 [==============================] - 167s 35ms/step - loss: 0.3536 - acc: 0.9354 - val_loss: 1.7844 - val_acc: 0.7058\n",
      "Epoch 42/50\n",
      "4784/4784 [==============================] - 167s 35ms/step - loss: 0.1770 - acc: 0.9613 - val_loss: 3.2428 - val_acc: 0.5486\n",
      "Epoch 43/50\n",
      "4784/4784 [==============================] - 166s 35ms/step - loss: 0.1125 - acc: 0.9730 - val_loss: 1.8703 - val_acc: 0.6858\n",
      "Epoch 44/50\n",
      "4784/4784 [==============================] - 167s 35ms/step - loss: 0.1498 - acc: 0.9638 - val_loss: 1.3251 - val_acc: 0.7661\n",
      "Epoch 45/50\n",
      "4784/4784 [==============================] - 166s 35ms/step - loss: 0.0947 - acc: 0.9739 - val_loss: 1.6210 - val_acc: 0.7201\n",
      "Epoch 46/50\n",
      "4784/4784 [==============================] - 166s 35ms/step - loss: 0.1029 - acc: 0.9712 - val_loss: 1.2923 - val_acc: 0.7693\n",
      "Epoch 47/50\n",
      "4784/4784 [==============================] - 167s 35ms/step - loss: 0.1897 - acc: 0.9584 - val_loss: 4.2155 - val_acc: 0.4684\n",
      "Epoch 48/50\n",
      "4784/4784 [==============================] - 168s 35ms/step - loss: 0.1700 - acc: 0.9624 - val_loss: 3.5145 - val_acc: 0.5285\n",
      "Epoch 49/50\n",
      "4784/4784 [==============================] - 167s 35ms/step - loss: 0.1050 - acc: 0.9720 - val_loss: 2.5887 - val_acc: 0.6102\n",
      "Epoch 50/50\n",
      "4784/4784 [==============================] - 166s 35ms/step - loss: 0.4316 - acc: 0.9348 - val_loss: 2.4128 - val_acc: 0.6506\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd8188b2f28>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data,np.array(y_train),validation_data=(data_valid, y_valid), epochs= 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6913309266411104"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(data_valid)\n",
    "fpr, tpr, _ = metrics.roc_curve(np.array(y_valid), y_pred)\n",
    "auc = metrics.auc(fpr,tpr)\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('CNN5_GloVe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model.append('Collbert CNN Model with GloVe Embedding')\n",
    "AUC.append(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN + Stacked Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv_model():\n",
    "    model_conv = Sequential()\n",
    "    model_conv.add(Embedding(vocabulary_size, 100, input_length=max_leng , weights=[embedding_matrix], trainable=False))\n",
    "    model_conv.add(Dropout(0.2))\n",
    "    model_conv.add(Conv1D(64, 5, activation='relu'))\n",
    "    model_conv.add(MaxPooling1D(pool_size=4))\n",
    "    model_conv.add(Bidirectional(LSTM(100,recurrent_dropout= 0.2, return_sequences= True)))\n",
    "    model_conv.add(Bidirectional(LSTM(50,recurrent_dropout= 0.2)))\n",
    "    model_conv.add(Dense(1, activation='sigmoid'))\n",
    "    model_conv.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model_conv\n",
    "model_conv = create_conv_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4784 samples, validate on 10223 samples\n",
      "Epoch 1/10\n",
      "4784/4784 [==============================] - 458s 96ms/step - loss: 0.6832 - acc: 0.5429 - val_loss: 0.7461 - val_acc: 0.3744\n",
      "Epoch 2/10\n",
      "4784/4784 [==============================] - 453s 95ms/step - loss: 0.6687 - acc: 0.5763 - val_loss: 0.6128 - val_acc: 0.7445\n",
      "Epoch 3/10\n",
      "4784/4784 [==============================] - 453s 95ms/step - loss: 0.6633 - acc: 0.5857 - val_loss: 0.6946 - val_acc: 0.5689\n",
      "Epoch 4/10\n",
      "4784/4784 [==============================] - 453s 95ms/step - loss: 0.6527 - acc: 0.6108 - val_loss: 0.7006 - val_acc: 0.4906\n",
      "Epoch 5/10\n",
      "4784/4784 [==============================] - 453s 95ms/step - loss: 0.6458 - acc: 0.6231 - val_loss: 0.5403 - val_acc: 0.7498\n",
      "Epoch 6/10\n",
      "4784/4784 [==============================] - 454s 95ms/step - loss: 0.6258 - acc: 0.6472 - val_loss: 0.5197 - val_acc: 0.7805\n",
      "Epoch 7/10\n",
      "4784/4784 [==============================] - 453s 95ms/step - loss: 0.6242 - acc: 0.6461 - val_loss: 0.5828 - val_acc: 0.7187\n",
      "Epoch 8/10\n",
      "4784/4784 [==============================] - 453s 95ms/step - loss: 0.5925 - acc: 0.6816 - val_loss: 0.7490 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "4784/4784 [==============================] - 453s 95ms/step - loss: 0.5614 - acc: 0.7099 - val_loss: 0.6300 - val_acc: 0.6390\n",
      "Epoch 10/10\n",
      "4784/4784 [==============================] - 453s 95ms/step - loss: 0.5386 - acc: 0.7341 - val_loss: 0.7725 - val_acc: 0.5400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd80c2b1c50>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_conv.fit(data, np.array(y_train), validation_data=(data_valid, y_valid), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6159363964085272"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model_conv.predict_proba(data_valid)\n",
    "fpr, tpr, _ = metrics.roc_curve(np.array(y_valid), y_pred)\n",
    "auc = metrics.auc(fpr,tpr)\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('CNN_BiLSTM_GloVe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model.append('CNN-BiLSTM Model with GloVe Embedding')\n",
    "AUC.append(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR on BoW</td>\n",
       "      <td>0.699700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LR on BoW &amp; BoC</td>\n",
       "      <td>0.699682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NB on BoW &amp; BoC</td>\n",
       "      <td>0.548622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LR on TF-IDF of BoW &amp; BoC</td>\n",
       "      <td>0.658587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LR on BoC</td>\n",
       "      <td>0.571647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NN on BoW &amp; BoC</td>\n",
       "      <td>0.708844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LR on BoW &amp; BoC &amp; up to 3grams</td>\n",
       "      <td>0.706103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NN on BoW &amp; BoC &amp; up to 3grams</td>\n",
       "      <td>0.715649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Collbert CNN Model with Word2Vec Embedding</td>\n",
       "      <td>0.683944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CNN-BiLSTM Model with Word2Vec Embedding</td>\n",
       "      <td>0.637042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Collbert CNN Model with GloVe Embedding</td>\n",
       "      <td>0.691331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CNN-BiLSTM Model with GloVe Embedding</td>\n",
       "      <td>0.615936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Model       AUC\n",
       "0                                    LR on BoW  0.699700\n",
       "1                              LR on BoW & BoC  0.699682\n",
       "2                              NB on BoW & BoC  0.548622\n",
       "3                    LR on TF-IDF of BoW & BoC  0.658587\n",
       "4                                    LR on BoC  0.571647\n",
       "5                              NN on BoW & BoC  0.708844\n",
       "6               LR on BoW & BoC & up to 3grams  0.706103\n",
       "7               NN on BoW & BoC & up to 3grams  0.715649\n",
       "8   Collbert CNN Model with Word2Vec Embedding  0.683944\n",
       "9     CNN-BiLSTM Model with Word2Vec Embedding  0.637042\n",
       "10     Collbert CNN Model with GloVe Embedding  0.691331\n",
       "11       CNN-BiLSTM Model with GloVe Embedding  0.615936"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Results_DL = pd.DataFrame({'Model': Model, 'AUC': AUC})\n",
    "Results_BoW = pd.read_pickle('AUC_Models_BoW.pkl')\n",
    "Results = pd.concat([Results_BoW, Results_DL], axis = 0)\n",
    "Results.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions:\n",
    "\n",
    " * Naïve approach of BoW yields slightly better results than sophisticated models such as CNN and BiLSTM.\n",
    " \n",
    " * Adding up to 3-grams improves the result of BoW model.\n",
    "\n",
    " * Combination of BoW and BoC with up to 3-grams and one dense layer has the best performance.\n",
    "\n",
    " * Adding bag of polarized CUIs to the analysis does not improve the results. It could be caused by the high correlation between text and CUIs. Also, I have realized that cTAKES sometimes cannot catch polarization and negation. It is also reported by my classmates.\n",
    "\n",
    " * Collobert et al. (2011) CNN model outperforms CNN-BiLSTM model.\n",
    "\n",
    " * There is not much difference between the results of GloVe pretrained embedding and Word2Vec embedding of MIMIC-III notes. \n",
    "\n",
    " * LSTM and BiLSTM are prone to overfitting. Here you see that I am using only 10 epochs. This is because after certain numbers of epochs AUC drops due to overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    " * Collobert R, Weston J, Bottou L, Karlen M, Kavukcuoglu K, Kuksa P. Natural language processing (almost) from scratch. Journal of Machine Learning Research. 2011; 12(Aug):2493–2537. \n",
    " \n",
    " * https://towardsdatascience.com/introduction-to-clinical-natural-language-processing-predicting-hospital-readmission-with-1736d52bc709\n",
    " \n",
    " * https://www.researchgate.net/publication/306093564_Dimensional_Sentiment_Analysis_Using_a_Regional_CNN-LSTM_Model\n",
    "\n",
    " * https://arxiv.org/abs/1703.08705\n",
    "\n",
    " * https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5510858/\n",
    "\n",
    " * https://www.nature.com/articles/sdata201635\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
